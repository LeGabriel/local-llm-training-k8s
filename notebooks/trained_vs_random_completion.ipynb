{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained vs Random-Init GPT Completion\n",
    "\n",
    "This notebook loads a trained checkpoint and compares text generation against a random-init model with the exact same architecture.\n",
    "\n",
    "By default it looks for:\n",
    "- `../config.yaml`\n",
    "- `../step_002400.pt`\n",
    "\n",
    "If your files are elsewhere, update the two path variables in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/GabrielLevaillant/Desktop/Pet-Projects/local-llm-training-k8s\n",
      "Config path: /Users/GabrielLevaillant/Desktop/Pet-Projects/local-llm-training-k8s/runs/20260219_172720_a45befc_gpt-wikitext-local/config.yaml\n",
      "Checkpoint path: /Users/GabrielLevaillant/Desktop/Pet-Projects/local-llm-training-k8s/runs/20260219_172720_a45befc_gpt-wikitext-local/checkpoints/step_002400.pt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent if Path.cwd().name == \"notebooks\" else Path.cwd().resolve()\n",
    "if str(PROJECT_ROOT / \"src\") not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "CONFIG_PATH = Path(\"../config.yaml\")\n",
    "CHECKPOINT_PATH = Path(\"../step_002400.pt\")\n",
    "\n",
    "def resolve_existing_path(default_path: Path, fallback_pattern: str) -> Path:\n",
    "    candidate = (Path.cwd() / default_path).resolve()\n",
    "    if candidate.exists():\n",
    "        return candidate\n",
    "\n",
    "    matches = sorted(PROJECT_ROOT.rglob(fallback_pattern))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find {default_path} and no fallback match for pattern '{fallback_pattern}'.\"\n",
    "        )\n",
    "    return matches[-1]\n",
    "\n",
    "resolved_config_path = resolve_existing_path(CONFIG_PATH, \"config.yaml\")\n",
    "resolved_checkpoint_path = resolve_existing_path(CHECKPOINT_PATH, \"step_002400.pt\")\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Config path:\", resolved_config_path)\n",
    "print(\"Checkpoint path:\", resolved_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model adapter: GPTAdapter\n",
      "Model type: gpt\n",
      "Device: mps\n",
      "Checkpoint step: 2400\n"
     ]
    }
   ],
   "source": [
    "from llmtrain.config.schemas import RunConfig\n",
    "from llmtrain.registry import initialize_registries\n",
    "from llmtrain.registry.models import get_model_adapter\n",
    "\n",
    "with resolved_config_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    cfg_dict = yaml.safe_load(f)\n",
    "cfg = RunConfig.model_validate(cfg_dict)\n",
    "\n",
    "payload = torch.load(resolved_checkpoint_path, map_location=\"cpu\", weights_only=False)\n",
    "checkpoint_config = payload.get(\"config\")\n",
    "if isinstance(checkpoint_config, dict) and checkpoint_config != cfg.model_dump():\n",
    "    print(\"Warning: checkpoint config differs from config.yaml; using config.yaml for model build.\")\n",
    "\n",
    "initialize_registries()\n",
    "adapter_cls = get_model_adapter(cfg.model.name)\n",
    "adapter = adapter_cls()\n",
    "\n",
    "device_name = cfg.run.device\n",
    "if device_name == \"mps\" and not torch.backends.mps.is_available():\n",
    "    print(\"MPS requested but unavailable; falling back to CPU.\")\n",
    "    device_name = \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "\n",
    "trained_model = adapter.build_model(cfg)\n",
    "trained_model.load_state_dict(payload[\"model_state_dict\"])\n",
    "trained_model = trained_model.to(device).eval()\n",
    "\n",
    "random_model = adapter.build_model(cfg)\n",
    "random_model = random_model.to(device).eval()\n",
    "\n",
    "tokenizer = adapter.build_tokenizer(cfg)\n",
    "if tokenizer is None:\n",
    "    raise RuntimeError(\"Tokenizer is required for text generation in this notebook.\")\n",
    "\n",
    "print(\"Model adapter:\", adapter_cls.__name__)\n",
    "print(\"Model type:\", cfg.model.name)\n",
    "print(\"Device:\", device)\n",
    "print(\"Checkpoint step:\", payload.get(\"step\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count (trained): 40,691,328\n",
      "Parameter count (random):  40,691,328\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def generate_text(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 48,\n",
    "    temperature: float = 0.8,\n",
    "    top_k: int | None = 40,\n",
    "    seed: int = 1234,\n",
    ") -> str:\n",
    "    torch.manual_seed(seed)\n",
    "    if device.type == \"mps\":\n",
    "        # Keep MPS and CPU randomness aligned for repeatable sampling.\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "    encoded = tokenizer.encode(prompt)\n",
    "    x = torch.tensor([encoded], dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        x_cond = x[:, -cfg.model.block_size :]\n",
    "        with torch.no_grad():\n",
    "            logits = model(x_cond)\n",
    "            next_logits = logits[:, -1, :]\n",
    "\n",
    "        if temperature <= 0:\n",
    "            next_token = torch.argmax(next_logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            next_logits = next_logits / temperature\n",
    "            if top_k is not None and top_k > 0:\n",
    "                k = min(top_k, next_logits.size(-1))\n",
    "                values, _ = torch.topk(next_logits, k=k)\n",
    "                cutoff = values[:, -1].unsqueeze(-1)\n",
    "                next_logits = torch.where(\n",
    "                    next_logits < cutoff,\n",
    "                    torch.full_like(next_logits, float(\"-inf\")),\n",
    "                    next_logits,\n",
    "                )\n",
    "\n",
    "            probs = torch.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        x = torch.cat((x, next_token), dim=1)\n",
    "\n",
    "    return tokenizer.decode(x[0].tolist())\n",
    "\n",
    "print(\"Parameter count (trained):\", f\"{count_parameters(trained_model):,}\")\n",
    "print(\"Parameter count (random): \", f\"{count_parameters(random_model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt ===\n",
      "Here is a list of my favorite things\n",
      "\n",
      "=== Trained model completion ===\n",
      "Here is a list of my favorite things of the second @-\n",
      "\n",
      "=== Random-init model completion ===\n",
      "Here is a list of my favorite thingsdebdullahSurv TrinityAlbert\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Here is a list of my favorite things\"\n",
    "max_new_tokens = 5\n",
    "temperature = 0.8\n",
    "top_k = 40\n",
    "seed = 7\n",
    "\n",
    "trained_completion = generate_text(\n",
    "    trained_model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "random_completion = generate_text(\n",
    "    random_model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "print(\"=== Prompt ===\")\n",
    "print(prompt)\n",
    "print(\"\\n=== Trained model completion ===\")\n",
    "print(trained_completion)\n",
    "print(\"\\n=== Random-init model completion ===\")\n",
    "print(random_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top next-token candidates (trained):\n",
      "' .': 0.1339\n",
      "' ,': 0.0880\n",
      "' that': 0.0518\n",
      "' to': 0.0491\n",
      "' and': 0.0362\n",
      "' in': 0.0268\n",
      "\" '\": 0.0261\n",
      "' of': 0.0225\n",
      "' \"': 0.0216\n",
      "' as': 0.0143\n",
      "\n",
      "Top next-token candidates (random-init):\n",
      "' Sie': 0.0001\n",
      "' bookmark': 0.0001\n",
      "' stationed': 0.0001\n",
      "' things': 0.0001\n",
      "' Melissa': 0.0001\n",
      "' tsunami': 0.0001\n",
      "' happen': 0.0001\n",
      "' inspir': 0.0001\n",
      "' reprim': 0.0001\n",
      "' Finding': 0.0001\n"
     ]
    }
   ],
   "source": [
    "def top_next_tokens(model: torch.nn.Module, tokenizer, text: str, k: int = 10) -> list[tuple[str, float]]:\n",
    "    ids = tokenizer.encode(text)\n",
    "    x = torch.tensor([ids[-cfg.model.block_size :]], dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "    top_probs, top_ids = torch.topk(probs, k=min(k, probs.size(-1)), dim=-1)\n",
    "    out: list[tuple[str, float]] = []\n",
    "    for token_id, p in zip(top_ids[0].tolist(), top_probs[0].tolist()):\n",
    "        token_text = tokenizer.decode([token_id]).replace(\"\\n\", \"\\\\n\")\n",
    "        out.append((token_text, float(p)))\n",
    "    return out\n",
    "\n",
    "trained_top = top_next_tokens(trained_model, tokenizer, prompt, k=10)\n",
    "random_top = top_next_tokens(random_model, tokenizer, prompt, k=10)\n",
    "\n",
    "print(\"Top next-token candidates (trained):\")\n",
    "for token, prob in trained_top:\n",
    "    print(f\"{token!r}: {prob:.4f}\")\n",
    "\n",
    "print(\"\\nTop next-token candidates (random-init):\")\n",
    "for token, prob in random_top:\n",
    "    print(f\"{token!r}: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
