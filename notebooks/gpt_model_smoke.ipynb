{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e19467e8",
      "metadata": {},
      "source": [
        "# GPT model smoke notebook\n",
        "\n",
        "This notebook creates a small `GPT` model from `llmtrain.models.gpt`, prints parameter counts,\n",
        "and runs a tiny forward pass to show basic model information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "85a0832e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from llmtrain.models.gpt import GPT\n",
        "\n",
        "\n",
        "def count_parameters(model: torch.nn.Module) -> tuple[int, int, int]:\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    non_trainable = total - trainable\n",
        "    return total, trainable, non_trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "775fb2d2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model: GPT\n",
            "config: {'vocab_size': 256, 'block_size': 32, 'd_model': 64, 'n_layers': 2, 'n_heads': 4, 'd_ff': 256, 'dropout': 0.1, 'tie_embeddings': True}\n",
            "total parameters: 118,528\n",
            "trainable parameters: 118,528\n",
            "non-trainable parameters: 0\n"
          ]
        }
      ],
      "source": [
        "cfg = {\n",
        "    \"vocab_size\": 256,\n",
        "    \"block_size\": 32,\n",
        "    \"d_model\": 64,\n",
        "    \"n_layers\": 2,\n",
        "    \"n_heads\": 4,\n",
        "    \"d_ff\": 256,\n",
        "    \"dropout\": 0.1,\n",
        "    \"tie_embeddings\": True,\n",
        "}\n",
        "\n",
        "model = GPT(**cfg)\n",
        "\n",
        "total, trainable, non_trainable = count_parameters(model)\n",
        "\n",
        "print(\"model:\", model.__class__.__name__)\n",
        "print(\"config:\", cfg)\n",
        "print(\"total parameters:\", f\"{total:,}\")\n",
        "print(\"trainable parameters:\", f\"{trainable:,}\")\n",
        "print(\"non-trainable parameters:\", f\"{non_trainable:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7365e26b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids shape: (2, 16)\n",
            "attention_mask shape: (2, 16)\n",
            "logits shape: (2, 16, 256)\n",
            "logits dtype: torch.float32\n",
            "device: cpu\n",
            "contains NaN: False\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(7)\n",
        "\n",
        "batch_size = 2\n",
        "seqlen = 16\n",
        "\n",
        "input_ids = torch.randint(0, cfg[\"vocab_size\"], (batch_size, seqlen), dtype=torch.long)\n",
        "attention_mask = torch.ones((batch_size, seqlen), dtype=torch.long)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "print(\"input_ids shape:\", tuple(input_ids.shape))\n",
        "print(\"attention_mask shape:\", tuple(attention_mask.shape))\n",
        "print(\"logits shape:\", tuple(logits.shape))\n",
        "print(\"logits dtype:\", logits.dtype)\n",
        "print(\"device:\", next(model.parameters()).device)\n",
        "print(\"contains NaN:\", bool(torch.isnan(logits).any()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a2004b59",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "token embedding params: 16,384\n",
            "lm_head params: 16,384\n",
            "weights tied: True\n",
            "number of transformer blocks: 2\n"
          ]
        }
      ],
      "source": [
        "# Small extra info snapshot\n",
        "embedding_params = model.token_embedding.weight.numel()\n",
        "lm_head_params = model.lm_head.weight.numel()\n",
        "\n",
        "print(\"token embedding params:\", f\"{embedding_params:,}\")\n",
        "print(\"lm_head params:\", f\"{lm_head_params:,}\")\n",
        "print(\"weights tied:\", model.lm_head.weight.data_ptr() == model.token_embedding.weight.data_ptr())\n",
        "print(\"number of transformer blocks:\", len(model.blocks))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.11.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
